# Gradiente Descent
### BÃ¡rbara Flores

Gradient descent is a fundamental optimization algorithm used in machine learning and various fields of computational mathematics. It serves as the backbone for training machine learning models and finding optimal solutions to complex problems. At its core, gradient descent is an iterative method employed to minimize a given cost or loss function. This optimization technique plays a crucial role in improving model accuracy and efficiency.

The primary objective of gradient descent is to locate the minimum of a cost function by iteratively adjusting the model's parameters. It achieves this by taking small steps in the direction of the steepest descent of the cost function. The "gradient" in gradient descent represents the rate of change of the cost function concerning the model parameters. This gradient points towards the direction of the steepest ascent, and the algorithm inverts this direction to minimize the function.

In this context, the task is to modify and enhance the provided [input_unigram_pytorch.py](https://github.com/BarbaraPFloresRios/IDS703_NLP_NaturalLanguageProcessing/blob/main/GradientDescent/input_unigram_pytorch.py) script to visualize the application of gradient descent in optimizing a unigram language model.

You can find my completed work for this assignment in the file. [unigram_pytorch.py](https://github.com/BarbaraPFloresRios/IDS703_NLP_NaturalLanguageProcessing/blob/main/GradientDescent/unigram_pytorch.py)
